{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "047a3f67",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-14T17:43:42.349435Z",
     "iopub.status.busy": "2025-01-14T17:43:42.349009Z",
     "iopub.status.idle": "2025-01-14T17:43:58.897600Z",
     "shell.execute_reply": "2025-01-14T17:43:58.896370Z"
    },
    "papermill": {
     "duration": 16.556442,
     "end_time": "2025-01-14T17:43:58.900300",
     "exception": false,
     "start_time": "2025-01-14T17:43:42.343858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: imutils in /usr/local/lib/python3.10/dist-packages (0.5.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q git+https://github.com/tensorflow/docs\n",
    "!pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b9928f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T17:43:58.908968Z",
     "iopub.status.busy": "2025-01-14T17:43:58.908594Z",
     "iopub.status.idle": "2025-01-14T17:44:09.167499Z",
     "shell.execute_reply": "2025-01-14T17:44:09.166444Z"
    },
    "papermill": {
     "duration": 10.265327,
     "end_time": "2025-01-14T17:44:09.169396",
     "exception": false,
     "start_time": "2025-01-14T17:43:58.904069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5a80910",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T17:44:09.177982Z",
     "iopub.status.busy": "2025-01-14T17:44:09.177361Z",
     "iopub.status.idle": "2025-01-14T17:44:09.227688Z",
     "shell.execute_reply": "2025-01-14T17:44:09.226499Z"
    },
    "papermill": {
     "duration": 0.056457,
     "end_time": "2025-01-14T17:44:09.229484",
     "exception": false,
     "start_time": "2025-01-14T17:44:09.173027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos for training: 1520\n",
      "Total videos for testing: 380\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>video_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>1372</td>\n",
       "      <td>roadaccidents</td>\n",
       "      <td>data\\roadaccidents\\RoadAccidents023_x264.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>1110</td>\n",
       "      <td>normal</td>\n",
       "      <td>data\\normal\\Normal_Videos786_x264.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>590</td>\n",
       "      <td>normal</td>\n",
       "      <td>data\\normal\\Normal_Videos225_x264.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>1466</td>\n",
       "      <td>roadaccidents</td>\n",
       "      <td>data\\roadaccidents\\RoadAccidents118_x264.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>995</td>\n",
       "      <td>normal</td>\n",
       "      <td>data\\normal\\Normal_Videos655_x264.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>456</td>\n",
       "      <td>normal</td>\n",
       "      <td>data\\normal\\Normal_Videos079_x264.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>597</td>\n",
       "      <td>normal</td>\n",
       "      <td>data\\normal\\Normal_Videos232_x264.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>1187</td>\n",
       "      <td>normal</td>\n",
       "      <td>data\\normal\\Normal_Videos917_x264.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>1832</td>\n",
       "      <td>stealing</td>\n",
       "      <td>data\\stealing\\Stealing096_x264.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>161</td>\n",
       "      <td>assault</td>\n",
       "      <td>data\\assault\\Assault012_x264.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0          label                                    video_name\n",
       "777         1372  roadaccidents  data\\roadaccidents\\RoadAccidents023_x264.mp4\n",
       "1263        1110         normal         data\\normal\\Normal_Videos786_x264.mp4\n",
       "994          590         normal         data\\normal\\Normal_Videos225_x264.mp4\n",
       "140         1466  roadaccidents  data\\roadaccidents\\RoadAccidents118_x264.mp4\n",
       "669          995         normal         data\\normal\\Normal_Videos655_x264.mp4\n",
       "1425         456         normal         data\\normal\\Normal_Videos079_x264.mp4\n",
       "51           597         normal         data\\normal\\Normal_Videos232_x264.mp4\n",
       "231         1187         normal         data\\normal\\Normal_Videos917_x264.mp4\n",
       "1143        1832       stealing            data\\stealing\\Stealing096_x264.mp4\n",
       "577          161        assault              data\\assault\\Assault012_x264.mp4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"/kaggle/input/real-time-anomaly-detection-in-cctv-surveillance/data/train.csv\")\n",
    "test_df = pd.read_csv(\"/kaggle/input/real-time-anomaly-detection-in-cctv-surveillance/data/test.csv\")\n",
    "\n",
    "print(f\"Total videos for training: {len(train_df)}\")\n",
    "print(f\"Total videos for testing: {len(test_df)}\")\n",
    "\n",
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2281af8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T17:44:09.237999Z",
     "iopub.status.busy": "2025-01-14T17:44:09.237673Z",
     "iopub.status.idle": "2025-01-14T17:44:09.241922Z",
     "shell.execute_reply": "2025-01-14T17:44:09.240989Z"
    },
    "papermill": {
     "duration": 0.010256,
     "end_time": "2025-01-14T17:44:09.243470",
     "exception": false,
     "start_time": "2025-01-14T17:44:09.233214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f860dbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T17:44:09.252111Z",
     "iopub.status.busy": "2025-01-14T17:44:09.251776Z",
     "iopub.status.idle": "2025-01-14T17:44:09.258757Z",
     "shell.execute_reply": "2025-01-14T17:44:09.257709Z"
    },
    "papermill": {
     "duration": 0.013137,
     "end_time": "2025-01-14T17:44:09.260437",
     "exception": false,
     "start_time": "2025-01-14T17:44:09.247300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abade92e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T17:44:09.268996Z",
     "iopub.status.busy": "2025-01-14T17:44:09.268610Z",
     "iopub.status.idle": "2025-01-14T17:44:12.173470Z",
     "shell.execute_reply": "2025-01-14T17:44:12.172310Z"
    },
    "papermill": {
     "duration": 2.911355,
     "end_time": "2025-01-14T17:44:12.175527",
     "exception": false,
     "start_time": "2025-01-14T17:44:09.264172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8816732d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T17:44:12.186057Z",
     "iopub.status.busy": "2025-01-14T17:44:12.185673Z",
     "iopub.status.idle": "2025-01-14T17:44:12.210874Z",
     "shell.execute_reply": "2025-01-14T17:44:12.209672Z"
    },
    "papermill": {
     "duration": 0.032462,
     "end_time": "2025-01-14T17:44:12.212578",
     "exception": false,
     "start_time": "2025-01-14T17:44:12.180116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abuse', 'arrest', 'arson', 'assault', 'burglary', 'explosion', 'fighting', 'normal', 'roadaccidents', 'robbery', 'shooting', 'shoplifting', 'stealing', 'vandalism']\n"
     ]
    }
   ],
   "source": [
    "label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(train_df[\"label\"]))\n",
    "\n",
    "print(label_processor.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0b4c005",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T17:44:12.223622Z",
     "iopub.status.busy": "2025-01-14T17:44:12.223126Z",
     "iopub.status.idle": "2025-01-14T17:44:12.738198Z",
     "shell.execute_reply": "2025-01-14T17:44:12.737164Z"
    },
    "papermill": {
     "duration": 0.5227,
     "end_time": "2025-01-14T17:44:12.739911",
     "exception": false,
     "start_time": "2025-01-14T17:44:12.217211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features in train set: (1520, 20, 2048)\n",
      "Frame masks in train set: (1520, 20)\n"
     ]
    }
   ],
   "source": [
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video_name\"].values.tolist()\n",
    "    labels = df[\"label\"].values\n",
    "    labels = label_processor(labels[..., None]).numpy()\n",
    "\n",
    "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
    "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
    "    # masked with padding or not.\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        # Gather all its frames and add a batch dimension.\n",
    "        frames = load_video(os.path.join(root_dir, path))\n",
    "        frames = frames[None, ...]\n",
    "\n",
    "        # Initialize placeholders to store the masks and features of the current video.\n",
    "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        # Extract features from the frames of the current video.\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :]\n",
    "                )\n",
    "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "\n",
    "    return (frame_features, frame_masks), labels\n",
    "\n",
    "\n",
    "train_data, train_labels = prepare_all_videos(train_df, \"train\")\n",
    "test_data, test_labels = prepare_all_videos(test_df, \"test\")\n",
    "\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55c329ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T17:44:12.750516Z",
     "iopub.status.busy": "2025-01-14T17:44:12.750121Z",
     "iopub.status.idle": "2025-01-14T17:44:28.401535Z",
     "shell.execute_reply": "2025-01-14T17:44:28.400471Z"
    },
    "papermill": {
     "duration": 15.658442,
     "end_time": "2025-01-14T17:44:28.403113",
     "exception": false,
     "start_time": "2025-01-14T17:44:12.744671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.4609 - loss: 2.6323\n",
      "Epoch 1: val_loss improved from inf to 2.61297, saving model to /tmp/video_classifier.weights.h5\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 49ms/step - accuracy: 0.4618 - loss: 2.6321 - val_accuracy: 0.5022 - val_loss: 2.6130\n",
      "Epoch 2/10\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.5063 - loss: 2.6051\n",
      "Epoch 2: val_loss improved from 2.61297 to 2.58799, saving model to /tmp/video_classifier.weights.h5\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.5061 - loss: 2.6049 - val_accuracy: 0.5022 - val_loss: 2.5880\n",
      "Epoch 3/10\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.4690 - loss: 2.5827\n",
      "Epoch 3: val_loss improved from 2.58799 to 2.56380, saving model to /tmp/video_classifier.weights.h5\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.4698 - loss: 2.5824 - val_accuracy: 0.5022 - val_loss: 2.5638\n",
      "Epoch 4/10\n",
      "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5078 - loss: 2.5543\n",
      "Epoch 4: val_loss improved from 2.56380 to 2.53901, saving model to /tmp/video_classifier.weights.h5\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.5073 - loss: 2.5540 - val_accuracy: 0.5022 - val_loss: 2.5390\n",
      "Epoch 5/10\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.4990 - loss: 2.5311\n",
      "Epoch 5: val_loss improved from 2.53901 to 2.51559, saving model to /tmp/video_classifier.weights.h5\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.4990 - loss: 2.5309 - val_accuracy: 0.5022 - val_loss: 2.5156\n",
      "Epoch 6/10\n",
      "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4936 - loss: 2.5045\n",
      "Epoch 6: val_loss improved from 2.51559 to 2.49207, saving model to /tmp/video_classifier.weights.h5\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.4940 - loss: 2.5042 - val_accuracy: 0.5022 - val_loss: 2.4921\n",
      "Epoch 7/10\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5082 - loss: 2.4775\n",
      "Epoch 7: val_loss improved from 2.49207 to 2.46965, saving model to /tmp/video_classifier.weights.h5\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.5079 - loss: 2.4774 - val_accuracy: 0.5022 - val_loss: 2.4696\n",
      "Epoch 8/10\n",
      "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4913 - loss: 2.4622\n",
      "Epoch 8: val_loss improved from 2.46965 to 2.44777, saving model to /tmp/video_classifier.weights.h5\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.4917 - loss: 2.4616 - val_accuracy: 0.5022 - val_loss: 2.4478\n",
      "Epoch 9/10\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5066 - loss: 2.4335\n",
      "Epoch 9: val_loss improved from 2.44777 to 2.42652, saving model to /tmp/video_classifier.weights.h5\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.5064 - loss: 2.4334 - val_accuracy: 0.5022 - val_loss: 2.4265\n",
      "Epoch 10/10\n",
      "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4990 - loss: 2.4122\n",
      "Epoch 10: val_loss improved from 2.42652 to 2.40541, saving model to /tmp/video_classifier.weights.h5\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.4990 - loss: 2.4120 - val_accuracy: 0.5022 - val_loss: 2.4054\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5170 - loss: 2.3925\n",
      "Test accuracy: 50.0%\n"
     ]
    }
   ],
   "source": [
    "# Utility for our sequence model.\n",
    "def get_sequence_model():\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "    # Refer to the following tutorial to understand the significance of using `mask`:\n",
    "    # https://keras.io/api/layers/recurrent_layers/gru/\n",
    "    x = keras.layers.GRU(16, return_sequences=True)(\n",
    "        frame_features_input, mask=mask_input\n",
    "    )\n",
    "    x = keras.layers.GRU(8)(x)\n",
    "    x = keras.layers.Dropout(0.3)(x)\n",
    "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return rnn_model\n",
    "\n",
    "\n",
    "# Utility for running experiments.\n",
    "def run_experiment():\n",
    "    filepath = \"/tmp/video_classifier.weights.h5\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0], train_data[1]],\n",
    "        train_labels,\n",
    "        validation_split=0.3,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint],\n",
    "    )\n",
    "\n",
    "    seq_model.load_weights(filepath)\n",
    "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history, seq_model\n",
    "\n",
    "\n",
    "_, sequence_model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ef99d73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T17:44:28.431685Z",
     "iopub.status.busy": "2025-01-14T17:44:28.431330Z",
     "iopub.status.idle": "2025-01-14T17:44:28.439104Z",
     "shell.execute_reply": "2025-01-14T17:44:28.438270Z"
    },
    "papermill": {
     "duration": 0.023817,
     "end_time": "2025-01-14T17:44:28.440657",
     "exception": false,
     "start_time": "2025-01-14T17:44:28.416840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_single_video(frames):\n",
    "    frames = frames[None, ...]\n",
    "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    for i, batch in enumerate(frames):\n",
    "        video_length = batch.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH, video_length)\n",
    "        for j in range(length):\n",
    "            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
    "        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "    return frame_features, frame_mask\n",
    "\n",
    "\n",
    "def sequence_prediction(path):\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frames = load_video(os.path.join(\"test\", path))\n",
    "    frame_features, frame_mask = prepare_single_video(frames)\n",
    "    probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n",
    "\n",
    "    for i in np.argsort(probabilities)[::-1]:\n",
    "        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "    return frames\n",
    "\n",
    "\n",
    "# This utility is for visualization.\n",
    "# Referenced from:\n",
    "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
    "def to_gif(images):\n",
    "    converted_images = images.astype(np.uint8)\n",
    "    imageio.mimsave(\"animation.gif\", converted_images, duration=100)\n",
    "    return embed.embed_file(\"animation.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f7fe32c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T17:44:28.468690Z",
     "iopub.status.busy": "2025-01-14T17:44:28.468349Z",
     "iopub.status.idle": "2025-01-14T17:44:28.909920Z",
     "shell.execute_reply": "2025-01-14T17:44:28.908581Z"
    },
    "papermill": {
     "duration": 0.457579,
     "end_time": "2025-01-14T17:44:28.911857",
     "exception": false,
     "start_time": "2025-01-14T17:44:28.454278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test video path: data\\robbery\\Robbery120_x264.mp4\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372ms/step\n",
      "  normal: 11.52%\n",
      "  robbery:  8.69%\n",
      "  roadaccidents:  8.65%\n",
      "  burglary:  7.06%\n",
      "  stealing:  6.92%\n",
      "  vandalism:  6.44%\n",
      "  arrest:  6.40%\n",
      "  fighting:  6.39%\n",
      "  shoplifting:  6.36%\n",
      "  assault:  6.35%\n",
      "  abuse:  6.34%\n",
      "  shooting:  6.32%\n",
      "  arson:  6.28%\n",
      "  explosion:  6.28%\n"
     ]
    }
   ],
   "source": [
    "test_video = np.random.choice(test_df[\"video_name\"].values.tolist())\n",
    "print(f\"Test video path: {test_video}\")\n",
    "test_frames = sequence_prediction(test_video)\n",
    "# to_gif(test_frames[:MAX_SEQ_LENGTH])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2760272,
     "sourceId": 4769020,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 51.659099,
   "end_time": "2025-01-14T17:44:31.694665",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-14T17:43:40.035566",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
